"""
OpenAI LLM æœåŠ¡æ¨¡å—
æä¾›é€šç”¨çš„ OpenAI API è°ƒç”¨åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œä»»åŠ¡
"""

import os
import logging
import json
from typing import List, Dict, Optional, Union, Any
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
project_dir = os.path.dirname(current_dir)
env_path = os.path.join(project_dir, '.env')
load_dotenv(env_path)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OpenAIService:
    """OpenAI LLMæœåŠ¡ç±»"""
    
    # æ”¯æŒçš„æ¨¡å‹é…ç½®
    MODELS = {
        'gpt-5-nano': {
            'name': 'GPT-5 Nano',
            'max_tokens': 128000,
            'cost_per_1k_input': 0.05,
            'cost_per_1k_output': 0.40,
            'best_for': ['ç¿»è¯‘', 'è½»é‡çº§ä»»åŠ¡', 'å¿«é€Ÿå“åº”'],
            'reasoning_effort': ['minimal', 'low'],
            'text_verbosity': ['low', 'medium'],  # æ·»åŠ text verbosityæ”¯æŒ
            'status': 'stable',  # ä½¿ç”¨æ­£ç¡®çš„APIååº”è¯¥ç¨³å®š
            'fallback': 'gpt-4o-mini'
        },
        'gpt-5-mini': {
            'name': 'GPT-5 Mini', 
            'max_tokens': 128000,
            'cost_per_1k_input': 0.25,
            'cost_per_1k_output': 2.0,
            'best_for': ['å¤æ‚æ¨ç†', 'åˆ›æ„ç”Ÿæˆ', 'æ·±åº¦åˆ†æ'],
            'reasoning_effort': ['minimal', 'low', 'medium'],
            'text_verbosity': ['low', 'medium', 'high'],  # æ·»åŠ text verbosityæ”¯æŒ
            'status': 'stable',  # ä½¿ç”¨æ­£ç¡®çš„APIååº”è¯¥ç¨³å®š
            'fallback': 'gpt-4o'
        },
        'gpt-5': {
            'name': 'GPT-5',
            'max_tokens': 128000,
            'cost_per_1k_input': 1.25,
            'cost_per_1k_output': 10.0,
            'best_for': ['æœ€å¤æ‚ä»»åŠ¡', 'é«˜è´¨é‡å†…å®¹', 'ä¸“ä¸šåˆ†æ'],
            'reasoning_effort': ['minimal', 'low', 'medium', 'high']
        },
        'gpt-4o-mini': {
            'name': 'GPT-4o Mini (å¤‡ç”¨)',
            'max_tokens': 128000,
            'cost_per_1k_input': 0.00015,
            'cost_per_1k_output': 0.0006,
            'best_for': ['å¤‡ç”¨ç¿»è¯‘', 'è½»é‡çº§ä»»åŠ¡']
        },
        'gpt-4o': {
            'name': 'GPT-4o (å¤‡ç”¨)',
            'max_tokens': 128000,
            'cost_per_1k_input': 0.005,
            'cost_per_1k_output': 0.015,
            'best_for': ['å¤‡ç”¨å¤æ‚ä»»åŠ¡', 'åˆ›æ„ç”Ÿæˆ']
        }
    }
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–OpenAIæœåŠ¡
        
        Args:
            api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        if not self.api_key:
            logger.error("OpenAI API Keyæœªæ‰¾åˆ°ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY")
            return
        
        try:
            # 2025å¹´æ ‡å‡†åˆå§‹åŒ–æ–¹å¼ - å¢å¼ºé‡è¯•å’Œè¶…æ—¶é…ç½®
            from openai import OpenAI
            import httpx
            
            # åˆ›å»ºè‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯ï¼Œå¢å¼ºè¿æ¥é…ç½®
            http_client = httpx.Client(
                timeout=httpx.Timeout(60.0, connect=10.0),  # æ€»è¶…æ—¶60sï¼Œè¿æ¥è¶…æ—¶10s
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
                # æ³¨æ„: httpxä¸æ”¯æŒretrieså‚æ•°ï¼Œé‡è¯•ç”±OpenAIå®¢æˆ·ç«¯å¤„ç†
            )
            
            self.client = OpenAI(
                api_key=self.api_key,
                max_retries=5,  # å¢åŠ åˆ°5æ¬¡é‡è¯•
                timeout=60.0,   # å¢åŠ æ€»è¶…æ—¶æ—¶é—´
                http_client=http_client
            )
            logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (v1.101+) - å¢å¼ºé‡è¯•é…ç½®")
                
        except Exception as e:
            logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥OpenAIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def chat_completion(self,
                       messages: List[Dict[str, str]],
                       model: str = 'gpt-5-nano',
                       max_tokens: Optional[int] = None,
                       temperature: float = 0.7,
                       response_format: Optional[Dict] = None,
                       reasoning_effort: str = 'low') -> Dict[str, Any]:
        """
        é€šç”¨çš„èŠå¤©å®ŒæˆAPIè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•° (0-2)
            response_format: å“åº”æ ¼å¼ (å¦‚: {"type": "json_object"})
        
        Returns:
            Dict: APIå“åº”ç»“æœ
        """
        if not self.is_available():
            return {
                'success': False,
                'error': 'OpenAIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥API Keyé…ç½®',
                'content': None
            }
        
        if model not in self.MODELS:
            return {
                'success': False,
                'error': f'ä¸æ”¯æŒçš„æ¨¡å‹: {model}ï¼Œæ”¯æŒçš„æ¨¡å‹: {list(self.MODELS.keys())}',
                'content': None
            }
        
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                'model': model,
                'messages': messages
            }
            
            # GPT-5ç³»åˆ—ä¸æ”¯æŒtemperatureå‚æ•°
            if not model.startswith('gpt-5'):
                request_params['temperature'] = temperature
            
            if max_tokens:
                # GPT-5ç³»åˆ—ä½¿ç”¨max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨max_tokens
                if model.startswith('gpt-5'):
                    request_params['max_completion_tokens'] = max_tokens
                else:
                    request_params['max_tokens'] = max_tokens
                
            if response_format:
                request_params['response_format'] = response_format
            
            # GPT-5ç³»åˆ—æ”¯æŒreasoning_effortå‚æ•°
            if model.startswith('gpt-5') and reasoning_effort:
                model_config = self.MODELS.get(model, {})
                supported_efforts = model_config.get('reasoning_effort', [])
                if reasoning_effort in supported_efforts:
                    request_params['reasoning_effort'] = reasoning_effort
            
            # è°ƒç”¨API
            response = self.client.chat.completions.create(**request_params)
            
            return {
                'success': True,
                'content': response.choices[0].message.content,
                'model_used': model,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                }
            }
            
        except Exception as e:
            return self._handle_api_error(e, model, request_params)
    
    def _handle_api_error(self, e: Exception, model: str, request_params: dict) -> Dict[str, Any]:
        """ç»Ÿä¸€å¤„ç†APIé”™è¯¯"""
        error_type = type(e).__name__
        error_message = str(e)
        
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥ - é”™è¯¯ç±»å‹: {error_type}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {error_message}")
        logger.error(f"ä½¿ç”¨æ¨¡å‹: {model}")
        
        # åˆ†ç±»é”™è¯¯å¹¶æä¾›è§£å†³æ–¹æ¡ˆ
        if 'Connection' in error_message or 'connection' in error_message.lower():
            detailed_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: {error_message}. è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒOpenAI APIæœåŠ¡çŠ¶æ€"
            retry_suggested = True
        elif 'timeout' in error_message.lower() or 'timed out' in error_message.lower():
            detailed_error = f"è¯·æ±‚è¶…æ—¶: {error_message}. è¯·ç¨åé‡è¯•æˆ–å‡å°‘è¯·æ±‚å¤æ‚åº¦"
            retry_suggested = True
        elif 'API key' in error_message or 'Unauthorized' in error_message:
            detailed_error = f"APIå¯†é’¥é”™è¯¯: {error_message}. è¯·æ£€æŸ¥OPENAI_API_KEYé…ç½®"
            retry_suggested = False
        elif 'Rate limit' in error_message or 'rate_limit' in error_message.lower():
            detailed_error = f"APIè°ƒç”¨é¢‘ç‡é™åˆ¶: {error_message}. è¯·ç¨åé‡è¯•"
            retry_suggested = True
        elif 'model' in error_message.lower():
            detailed_error = f"æ¨¡å‹é”™è¯¯: {error_message}. æ¨¡å‹'{model}'å¯èƒ½ä¸å¯ç”¨"
            retry_suggested = False
        elif 'overloaded' in error_message.lower() or 'capacity' in error_message.lower():
            detailed_error = f"æœåŠ¡è¿‡è½½: {error_message}. OpenAIæœåŠ¡å½“å‰ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•"
            retry_suggested = True
        else:
            detailed_error = f"æœªçŸ¥é”™è¯¯: {error_message}"
            retry_suggested = True
        
        logger.error(f"è¯¦ç»†é”™è¯¯è¯´æ˜: {detailed_error}")
        if retry_suggested:
            logger.info("ğŸ’¡ å»ºè®®: å¯ä»¥é‡è¯•æ­¤è¯·æ±‚")
        
        # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"å¤±è´¥çš„è¯·æ±‚å‚æ•°: {json.dumps(request_params, ensure_ascii=False, indent=2)}")
        
        return {
            'success': False,
            'error': detailed_error,
            'error_type': error_type,
            'original_error': error_message,
            'retry_suggested': retry_suggested,
            'content': None
        }
    
    def gpt5_responses_create(self,
                            input_text: str,
                            model: str = 'gpt-5-nano',
                            instructions: str = None,
                            reasoning_effort: str = 'low') -> Dict[str, Any]:
        """
        ä½¿ç”¨æ­£ç¡®çš„GPT-5 responses.create APIæ ¼å¼
        åŸºäºOpenAIå®˜æ–¹cookbookç¤ºä¾‹
        """
        if not self.is_available():
            return {
                'success': False,
                'error': 'OpenAIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥API Keyé…ç½®',
                'content': None
            }
        
        try:
            # æ„å»ºGPT-5è¯·æ±‚å‚æ•° - åŸºäºæˆåŠŸçš„æµ‹è¯•ä»£ç 
            request_params = {
                'model': model,
                'input': input_text
            }
            
            # æ·»åŠ æ¨ç†åŠªåŠ›å‚æ•° - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
            if reasoning_effort in ['minimal', 'low', 'medium', 'high']:
                request_params['reasoning'] = {'effort': reasoning_effort}
            else:
                # é»˜è®¤ä½¿ç”¨minimalä»¥æé«˜å“åº”é€Ÿåº¦
                request_params['reasoning'] = {'effort': 'minimal'}
            
            # æ·»åŠ textå‚æ•° - ä½¿ç”¨æ¨èçš„mediumæ¨¡å¼
            request_params['text'] = {'verbosity': 'medium'}
            
            # æ·»åŠ æŒ‡ä»¤å‚æ•°
            if instructions:
                request_params['instructions'] = instructions
            
            logger.info(f"ğŸ¯ GPT-5 APIè°ƒç”¨: {model}, reasoning_effort={reasoning_effort}")
            
            # è°ƒç”¨GPT-5ä¸“ç”¨API
            response = self.client.responses.create(**request_params)
            
            # åŸºäºæˆåŠŸæµ‹è¯•ä»£ç çš„å“åº”è§£æé€»è¾‘
            output_texts = []
            if hasattr(response, 'output') and response.output:
                for message in response.output:
                    if hasattr(message, 'content') and message.content:
                        for content_item in message.content:
                            if hasattr(content_item, 'text') and content_item.text:
                                output_texts.append(content_item.text)
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹
            content = '\n'.join(output_texts) if output_texts else None
            
            # è¯¦ç»†æ—¥å¿—è®°å½•å“åº”å†…å®¹
            logger.info(f"ğŸ” GPT-5å“åº”è§£æè¯¦æƒ…: {model}")
            logger.info(f"   æ‰¾åˆ°çš„æ–‡æœ¬æ®µæ•°: {len(output_texts)}")
            logger.info(f"   å“åº”å†…å®¹é•¿åº¦: {len(content) if content else 0}å­—ç¬¦")
            if content:
                logger.info(f"   å“åº”å†…å®¹é¢„è§ˆ: {content[:200]}...")
            else:
                logger.warning(f"   âš ï¸ å“åº”å†…å®¹ä¸ºç©º!")
                # å°è¯•è¾“å‡ºåŸå§‹å“åº”ç»“æ„ç”¨äºè°ƒè¯•
                logger.info(f"   åŸå§‹å“åº”ç»“æ„: {type(response)}")
                if hasattr(response, 'output'):
                    logger.info(f"   response.outputç±»å‹: {type(response.output)}")
                    logger.info(f"   response.outputé•¿åº¦: {len(response.output) if response.output else 0}")
            
            if not content:
                logger.warning(f"âš ï¸ GPT-5å“åº”ä¸ºç©º: {model}")
                return {
                    'success': False,
                    'error': 'GPT-5è¿”å›ç©ºå“åº”',
                    'content': None
                }
            
            return {
                'success': True,
                'content': content,
                'model_used': model,
                'usage': getattr(response, 'usage', {})
            }
            
        except Exception as e:
            return self._handle_api_error(e, model, request_params)
    
    def chat_completion_with_retry(self,
                                 messages: List[Dict[str, str]],
                                 model: str = 'gpt-5-nano',
                                 max_tokens: Optional[int] = None,
                                 temperature: float = 0.7,
                                 response_format: Optional[Dict] = None,
                                 reasoning_effort: str = 'low',
                                 max_manual_retries: int = 2) -> Dict[str, Any]:
        """
        å¸¦æ‰‹åŠ¨é‡è¯•é€»è¾‘çš„chat_completion
        åœ¨OpenAIå®¢æˆ·ç«¯é‡è¯•å¤±è´¥åï¼Œè¿›è¡Œåº”ç”¨å±‚é‡è¯•
        """
        import time
        
        last_error = None
        retry_delays = [1, 2, 4]  # æŒ‡æ•°é€€é¿ï¼š1ç§’, 2ç§’, 4ç§’
        
        for attempt in range(max_manual_retries + 1):
            try:
                if attempt > 0:
                    delay = retry_delays[min(attempt - 1, len(retry_delays) - 1)]
                    logger.info(f"ğŸ”„ ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
                
                result = self.chat_completion(
                    messages=messages,
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    response_format=response_format,
                    reasoning_effort=reasoning_effort
                )
                
                if result['success']:
                    if attempt > 0:
                        logger.info(f"âœ… é‡è¯•æˆåŠŸï¼ç¬¬ {attempt + 1} æ¬¡å°è¯•")
                    return result
                else:
                    last_error = result
                    # æ£€æŸ¥æ˜¯å¦å»ºè®®é‡è¯•
                    if not result.get('retry_suggested', True):
                        logger.warning("âŒ é”™è¯¯ä¸å»ºè®®é‡è¯•ï¼Œåœæ­¢é‡è¯•")
                        break
                    
            except Exception as e:
                last_error = {
                    'success': False,
                    'error': f"é‡è¯•è¿‡ç¨‹ä¸­å¼‚å¸¸: {str(e)}",
                    'error_type': type(e).__name__,
                    'original_error': str(e),
                    'content': None
                }
        
        logger.error(f"âŒ æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥äº† (å…± {max_manual_retries + 1} æ¬¡)")
        return last_error or {
            'success': False,
            'error': 'æœªçŸ¥çš„é‡è¯•å¤±è´¥',
            'content': None
        }
    
    def translate_to_chinese(self, 
                           texts: Union[str, List[str]], 
                           model: str = 'gpt-5-nano',
                           preserve_context: bool = True,
                           source_language: str = 'English',
                           country_code: Optional[str] = None) -> Dict[str, Any]:
        """
        å°†æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡
        
        Args:
            texts: è¦ç¿»è¯‘çš„æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹
            preserve_context: æ˜¯å¦ä¿æŒè¯­å¢ƒ(å¹¿å‘Š/è¥é”€ç›¸å…³)
            source_language: æºè¯­è¨€ï¼ˆå¦‚ï¼š'Tiáº¿ng Viá»‡t', 'à¸ aà¸©à¸²à¹„à¸—à¸¢', 'English'ç­‰ï¼‰
            country_code: å›½å®¶ä»£ç ï¼Œç”¨äºè‡ªåŠ¨è¯†åˆ«æºè¯­è¨€
        
        Returns:
            Dict: ç¿»è¯‘ç»“æœ
        """
        if isinstance(texts, str):
            texts = [texts]
        
        if not texts:
            return {'success': True, 'translations': []}
        
        # å›½å®¶ä»£ç åˆ°æœ¬åœ°è¯­è¨€åç§°çš„æ˜ å°„
        local_language_map = {
            'VN': 'Tiáº¿ng Viá»‡t',
            'TH': 'à¸ à¸²à¸©à¸²à¹„à¸—à¸¢', 
            'SG': 'English',
            'MY': 'English',
            'ID': 'Bahasa Indonesia',
            'PH': 'English'
        }
        
        # æ ¹æ®å›½å®¶ä»£ç ç¡®å®šæºè¯­è¨€
        if country_code and country_code in local_language_map:
            detected_language = local_language_map[country_code]
        else:
            detected_language = source_language
        
        texts_str = '\n'.join([f"{i+1}. {text}" for i, text in enumerate(texts)])
        
        messages = [
            {
                "role": "system", 
                "content": f"You are an expert linguist, specializing in translation from {detected_language} to ç®€ä½“ä¸­æ–‡. translate directly without explanation"
            },
            {
                "role": "user",
                "content": f"è¯·ç¿»è¯‘ä»¥ä¸‹{len(texts)}ä¸ªçƒ­é—¨è¯é¢˜ï¼š\n\n{texts_str}"
            }
        ]
        
        # å¯¹äºGPT-5æ¨¡å‹ï¼Œä½¿ç”¨ä¼˜åŒ–çš„responses.createæ–¹æ³•
        if model.startswith('gpt-5'):
            # æ„å»ºGPT-5æ ¼å¼çš„ç¿»è¯‘prompt
            gpt5_prompt = f"""è¯·ç¿»è¯‘ä»¥ä¸‹{len(texts)}ä¸ªçƒ­é—¨è¯é¢˜ä¸ºå‡†ç¡®çš„ç®€ä½“ä¸­æ–‡ï¼š

{texts_str}

è¯·ç¡®ä¿ç¿»è¯‘å‡†ç¡®ã€è‡ªç„¶ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ã€‚"""
            
            result_gpt5 = self.gpt5_responses_create(
                input_text=gpt5_prompt,
                model=model,
                instructions=f"ä½ æ˜¯ä¸“ä¸šçš„è¯­è¨€å­¦å®¶ï¼Œæ“…é•¿ä»{detected_language}ç¿»è¯‘åˆ°ç®€ä½“ä¸­æ–‡ã€‚è¯·ç›´æ¥ç¿»è¯‘ï¼Œæ— éœ€è§£é‡Šã€‚",
                reasoning_effort='minimal'  # ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨æœ€å°æ¨ç†
            )
            
            if result_gpt5['success']:
                result = result_gpt5
            else:
                # GPT-5å¤±è´¥æ—¶é™çº§åˆ°GPT-4o
                logger.warning(f"GPT-5ç¿»è¯‘å¤±è´¥ï¼Œé™çº§åˆ°å¤‡ç”¨æ¨¡å‹")
                result = self.chat_completion(
                    messages=messages,
                    model='gpt-4o-mini',
                    max_tokens=1000,
                    temperature=0.3,
                    reasoning_effort='minimal'
                )
        else:
            result = self.chat_completion(
                messages=messages,
                model=model,
                max_tokens=1000,
                temperature=0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿ç¿»è¯‘ä¸€è‡´æ€§
                reasoning_effort='minimal'  # ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨æœ€å°æ¨ç†
            )
        
        if result['success']:
            # è§£æç¿»è¯‘ç»“æœ - æ›´å¥å£®çš„å¤„ç†
            raw_content = result['content'].strip()
            translations = []
            
            # æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
            lines = raw_content.split('\n')
            for line in lines:
                cleaned_line = line.strip()
                # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€ï¼ˆå¦‚"1. ", "- ", "â€¢ "ç­‰ï¼‰
                cleaned_line = cleaned_line.lstrip('0123456789.- â€¢\t')
                if cleaned_line:
                    translations.append(cleaned_line)
            
            # ç¡®ä¿ç¿»è¯‘æ•°é‡åŒ¹é…
            if len(translations) != len(texts):
                logger.warning(f"æ‰¹é‡ç¿»è¯‘æ•°é‡ä¸åŒ¹é…: åŸæ–‡{len(texts)}ä¸ªï¼Œè¯‘æ–‡{len(translations)}ä¸ª")
                logger.debug(f"åŸæ–‡: {texts}")
                logger.debug(f"è¯‘æ–‡: {translations}")
                
                # æ™ºèƒ½ä¿®å¤ï¼šè¡¥é½æˆ–æˆªå–
                if len(translations) < len(texts):
                    # è¡¥é½ç¼ºå¤±çš„ç¿»è¯‘ï¼ˆä½¿ç”¨åŸæ–‡ï¼‰
                    missing_count = len(texts) - len(translations)
                    logger.warning(f"è¡¥é½{missing_count}ä¸ªç¼ºå¤±çš„ç¿»è¯‘")
                    for i in range(missing_count):
                        idx = len(translations) + i
                        if idx < len(texts):
                            translations.append(texts[idx])
                else:
                    # æˆªå–å¤šä½™çš„ç¿»è¯‘
                    translations = translations[:len(texts)]
                    logger.warning(f"æˆªå–åˆ°{len(texts)}ä¸ªç¿»è¯‘ç»“æœ")
            
            logger.info(f"âœ… æ‰¹é‡ç¿»è¯‘æˆåŠŸ: {len(texts)}ä¸ªè¯é¢˜ -> {len(translations)}ä¸ªä¸­æ–‡ç»“æœ")
            
            return {
                'success': True,
                'translations': translations,
                'original_texts': texts,
                'model_used': result['model_used'],
                'usage': result['usage'],
                'batch_size': len(texts)
            }
        else:
            return {
                'success': False,
                'error': result['error'],
                'translations': texts,  # è¿”å›åŸæ–‡ä½œä¸ºå¤‡ç”¨
                'fallback': True
            }
    
    def generate_creative_content(self,
                                prompt_data: Dict[str, Any] = None,
                                prompt: str = None,
                                style: str = "professional",
                                model: str = 'gpt-5-mini',
                                max_tokens: int = 4000) -> str:
        """
        ç”Ÿæˆåˆ›æ„å†…å®¹ï¼Œæ”¯æŒç»“æ„åŒ–çš„JSON promptå’Œä¼ ç»Ÿæ–‡æœ¬prompt
        
        Args:
            prompt_data: ç»“æ„åŒ–çš„JSON promptæ•°æ®
            prompt: ä¼ ç»Ÿæ–‡æœ¬æç¤ºï¼ˆä¸prompt_dataäºŒé€‰ä¸€ï¼‰
            style: é£æ ¼ (professional, casual, creative, etc.)
            model: ä½¿ç”¨çš„æ¨¡å‹
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
        
        Returns:
            str: ç”Ÿæˆçš„åˆ›æ„å†…å®¹ï¼ˆJSONå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        """
        if prompt_data:
            # ä½¿ç”¨ä¼˜åŒ–çš„JSON prompt - éµå¾ªOpenAIæœ€ä½³å®è·µ
            
            # æå–å’Œç®€åŒ–æ•°æ®ï¼Œè®¾ç½®é»˜è®¤å€¼
            user_idea = prompt_data.get("user_input", {}).get("idea", "")
            custom_inputs = prompt_data.get("user_input", {}).get("custom_inputs", {})
            selected_dimensions = prompt_data.get("selected_dimensions", {})
            count = prompt_data.get("requirements", {}).get("count", 5)
            
            # ä»UIè·å–æˆ–è®¾ç½®é»˜è®¤å€¼ - æ”¹ä¸ºå¯é€‰å­—æ®µ
            target_region = custom_inputs.get("target_region", "").strip()
            game_style = custom_inputs.get("game_style", "").strip()
            age_group = custom_inputs.get("age_group", "").strip()
            
            # å¤„ç†ç©ºå€¼æƒ…å†µ
            if not user_idea.strip():
                user_idea = "ä¸€æ¬¾å…·æœ‰åˆ›æ–°ç©æ³•çš„ç§»åŠ¨æ¸¸æˆ"
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåœ°åŒºï¼Œä½¿ç”¨é»˜è®¤
            if not target_region:
                target_region = "è¶Šå—"  # ä¿ç•™åœ°åŒºä½œä¸ºå¿…éœ€å­—æ®µ
            
            # æ„å»ºç®€åŒ–çš„ç»´åº¦ä¿¡æ¯
            dimension_summary = []
            for dim_name, options in selected_dimensions.items():
                if options:  # åªåŒ…å«æœ‰é€‰é¡¹çš„ç»´åº¦
                    option_names = [opt.get("name", "") for opt in options]
                    dimension_summary.append(f"{dim_name}: {', '.join(option_names)}")
            
            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸“ä¸šçš„æ¸¸æˆå¹¿å‘Šå›¾ç‰‡åˆ›æ„ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æ¸¸æˆèƒŒæ™¯ç”Ÿæˆä¸°å¯Œè¯¦ç»†çš„é™æ€å›¾ç‰‡å¹¿å‘Šåˆ›æ„æè¿°ã€‚

åˆ›ä½œè¦æ±‚ï¼š
- æ¯ä¸ªåˆ›æ„éƒ½è¦æœ‰ç‹¬ç‰¹æ€§å’Œåˆ›æ–°æ€§
- æä¾›è¯¦ç»†ä¸°å¯Œçš„è§†è§‰æè¿°ï¼ŒåŒ…æ‹¬å…·ä½“çš„åœºæ™¯ã€äººç‰©ã€åŠ¨ä½œã€æƒ…æ„Ÿç­‰
- é•œå¤´å’Œå…‰çº¿æè¿°è¦ä¸“ä¸šï¼ŒåŒ…æ‹¬æ‹æ‘„è§’åº¦ã€å…‰çº¿ç±»å‹ã€æ„å›¾æ–¹å¼ç­‰
- è‰²å½©å’Œé“å…·æè¿°è¦å…·ä½“ï¼ŒåŒ…æ‹¬è‰²å½©æ­é…ã€æè´¨è´¨æ„Ÿã€é“å…·ç»†èŠ‚ç­‰
- ç¡®ä¿å†…å®¹é€‚åˆè¶Šå—å¸‚åœºçš„æ–‡åŒ–å’Œå®¡ç¾åå¥½
- æ¯ä¸ªåˆ›æ„éƒ½è¦èƒ½å¤Ÿè®©AIå›¾åƒç”Ÿæˆå·¥å…·å‡†ç¡®ç†è§£å¹¶ç”Ÿæˆé«˜è´¨é‡å›¾ç‰‡

æŠ€æœ¯è¦æ±‚ï¼š
- å¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
- ä¸¥ç¦åœ¨ç”»é¢ä¸­å‡ºç°ä»»ä½•æ–‡å­—ã€Logoã€å­—å¹•ä¸æ ‡è¯†

JSONæ ¼å¼ï¼š
{
  "creatives": [
    {
      "core_concept": "æ ¸å¿ƒæ¦‚å¿µ",
      "scene_description": "å»ºè®®å›¾ç‰‡çš„ç”»é¢æè¿°",
      "camera_lighting": "é•œå¤´/å…‰çº¿å¤„ç†",
      "color_props": "è‰²å½©ä¸é“å…·ç­‰ç»†èŠ‚",
      "key_notes": "å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆç»Ÿä¸€å¼ºè°ƒï¼šç”»é¢ä¸­ä¸¥ç¦å‡ºç°ä»»ä½•æ–‡å­—ã€Logoã€å­—å¹•ä¸æ ‡è¯†ï¼‰"
    }
  ]
}"""
                },
                {
                    "role": "user", 
                    "content": f"""è¯·ç”Ÿæˆ{count}ç»„åœ¨è¶Šå—è½åœ°çš„{user_idea}é™æ€å›¾ç‰‡å¹¿å‘Šåˆ›æ„ã€‚æ¯ç»„éƒ½åŒ…å«ï¼šæ ¸å¿ƒæ¦‚å¿µã€å»ºè®®å›¾ç‰‡çš„ç”»é¢ã€é•œå¤´/å…‰çº¿å¤„ç†ã€è‰²å½©ä¸é“å…·ç­‰ç»†èŠ‚ï¼Œä¾¿äºAIç”Ÿæˆå›¾ç‰‡ã€å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆç»Ÿä¸€å¼ºè°ƒï¼šç”»é¢ä¸­ä¸¥ç¦å‡ºç°ä»»ä½•æ–‡å­—ã€Logoã€å­—å¹•ä¸æ ‡è¯†ï¼‰ã€‚

ç”¨æˆ·æ¸¸æˆèƒŒæ™¯ï¼š{user_idea}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆ{count}ç»„ä¸åŒçš„å›¾ç‰‡åˆ›æ„æè¿°ï¼Œç¡®ä¿æ¯ç»„éƒ½æœ‰ç‹¬ç‰¹æ€§å’Œè§†è§‰å¸å¼•åŠ›ï¼Œç¬¦åˆè¶Šå—å¸‚åœºç‰¹ç‚¹ã€‚"""
                }
            ]
            
            # è¯¦ç»†æ—¥å¿—æ‰“å° - è¿è¡Œæ—¶æ£€æŸ¥ç”¨
            logger.info("="*80)
            logger.info("ğŸ¨ å›¾ç‰‡åˆ›æ„ç”Ÿæˆ - å®Œæ•´Promptå†…å®¹")
            logger.info("="*80)
            logger.info("ğŸ“‹ è¾“å…¥å‚æ•°:")
            logger.info(f"   ç”¨æˆ·æƒ³æ³•: {user_idea}")
            logger.info(f"   ç›®æ ‡åœ°åŒº: {target_region}")
            logger.info(f"   æ¸¸æˆé£æ ¼: {game_style}")
            logger.info(f"   ç›®æ ‡å¹´é¾„: {age_group}")
            logger.info(f"   ç”Ÿæˆæ•°é‡: {count}")
            logger.info(f"   ä½¿ç”¨æ¨¡å‹: {model}")
            logger.info(f"   é€‰æ‹©ç»´åº¦: {len(selected_dimensions)}ä¸ªç»´åº¦ï¼Œ{sum(len(opts) for opts in selected_dimensions.values())}ä¸ªé€‰é¡¹")
            
            logger.info("\nğŸ“¨ System Message:")
            logger.info("-" * 60)
            logger.info(messages[0]['content'])
            
            logger.info("\nğŸ“¨ User Message:")
            logger.info("-" * 60)
            logger.info(messages[1]['content'])
            
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬prompt
            style_prompts = {
                'professional': 'è¯·ç”¨ä¸“ä¸šã€æ­£å¼çš„è¯­è°ƒ',
                'casual': 'è¯·ç”¨è½»æ¾ã€äº²åˆ‡çš„è¯­è°ƒ',
                'creative': 'è¯·ç”¨å¯Œæœ‰åˆ›æ„ã€æƒ³è±¡åŠ›çš„è¯­è°ƒ',
                'persuasive': 'è¯·ç”¨æœ‰è¯´æœåŠ›ã€å¸å¼•äººçš„è¯­è°ƒ'
            }
            
            style_instruction = style_prompts.get(style, style_prompts['professional'])
            
            messages = [
                {
                    "role": "system",
                    "content": f"""ä½ æ˜¯ä¸“ä¸šçš„å¹¿å‘Šåˆ›æ„ä¸“å®¶ã€‚{style_instruction}ç”Ÿæˆå†…å®¹ã€‚
                    
è¦æ±‚ï¼š
1. å†…å®¹è¦æœ‰å¸å¼•åŠ›å’Œåˆ›æ„æ€§
2. é€‚åˆç›®æ ‡å—ä¼—
3. ç¬¦åˆå¹¿å‘Šè¥é”€è§„èŒƒ
4. è¯­è¨€å…·æœ‰å¸å¼•åŠ›å’Œæ„ŸæŸ“åŠ›"""
                },
                {
                    "role": "user",
                    "content": prompt or "è¯·ç”Ÿæˆä¸€äº›åˆ›æ„å†…å®¹"
                }
            ]
        
        # åŠ¨æ€è®¡ç®—max_tokens - æ ¹æ®ç”Ÿæˆæ•°é‡å’Œpromptç±»å‹è°ƒæ•´
        if prompt_data:
            # å›¾ç‰‡åˆ›æ„ç”Ÿæˆéœ€è¦æ›´å¤štokens
            estimated_tokens_per_creative = 200  # æ¯ä¸ªåˆ›æ„å¤§çº¦200 tokens
            base_tokens = 500  # åŸºç¡€overhead
            dynamic_max_tokens = base_tokens + (count * estimated_tokens_per_creative)
            # ç¡®ä¿åœ¨æ¨¡å‹é™åˆ¶å†…ï¼Œæœ€å°‘2000ï¼Œæœ€å¤š8000
            dynamic_max_tokens = min(max(dynamic_max_tokens, 2000), 8000)
            actual_max_tokens = dynamic_max_tokens
        else:
            actual_max_tokens = max_tokens
            
        logger.info(f"ğŸ¯ Tokené…ç½®: ç”Ÿæˆ{count if prompt_data else '?'}ä¸ªåˆ›æ„ï¼Œä½¿ç”¨{actual_max_tokens} max_tokens")
        
        # æ˜¾ç¤ºæœ€ç»ˆAPIè°ƒç”¨å‚æ•°
        logger.info("\nğŸ¯ æœ€ç»ˆAPIè°ƒç”¨å‚æ•°:")
        logger.info(f"   model: {model}")
        logger.info(f"   max_tokens: {actual_max_tokens}")
        logger.info(f"   temperature: 0.8") 
        logger.info(f"   response_format: {'json_object' if prompt_data else 'text'}")
        logger.info(f"   reasoning_effort: medium")
        logger.info("="*80)
        
        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œè‡ªåŠ¨é™çº§
        result = self._generate_with_model_fallback(
            messages=messages,
            model=model,
            max_tokens=actual_max_tokens,
            prompt_data=prompt_data,
            reasoning_effort='medium'
        )
        
        if result['success']:
            return result['content']
        else:
            error_details = {
                'error': result.get('error', 'æœªçŸ¥é”™è¯¯'),
                'error_type': result.get('error_type', 'Unknown'),
                'original_error': result.get('original_error', ''),
                'model': model,
                'prompt_size': len(str(prompt_data)) if prompt_data else len(prompt or '')
            }
            
            # è®°å½•åˆ›æ„ç”Ÿæˆç‰¹å®šçš„é”™è¯¯ä¿¡æ¯
            logger.error(f"åˆ›æ„ç”Ÿæˆå¤±è´¥ - æ¨¡å‹: {model}")
            logger.error(f"é”™è¯¯ç±»å‹: {error_details['error_type']}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {error_details['error']}")
            
            raise Exception(f"AIåˆ›æ„ç”Ÿæˆå¤±è´¥ [{error_details['error_type']}]: {error_details['error']}")
    
    def _generate_with_model_fallback(self,
                                    messages: List[Dict[str, str]],
                                    model: str,
                                    max_tokens: int,
                                    prompt_data: Dict = None,
                                    reasoning_effort: str = 'medium') -> Dict[str, Any]:
        """
        å¸¦è‡ªåŠ¨é™çº§çš„åˆ›æ„ç”Ÿæˆ
        GPT-5 -> GPT-4o è‡ªåŠ¨åˆ‡æ¢
        """
        # é¦–å…ˆå°è¯•åŸå§‹æ¨¡å‹
        if model.startswith('gpt-5'):
            logger.info(f"ğŸ¯ å°è¯•ä½¿ç”¨GPT-5æ¨¡å‹: {model}")
            
            # å¯¹äºGPT-5ï¼Œå°è¯•ä½¿ç”¨æ­£ç¡®çš„responses.createæ ¼å¼
            try:
                # å°†messagesè½¬æ¢ä¸ºå•ä¸ªprompt
                if prompt_data:
                    # ä½¿ç”¨ç»“æ„åŒ–prompt
                    user_idea = prompt_data.get("user_input", {}).get("idea", "æ¸¸æˆåˆ›æ„")
                    custom_inputs = prompt_data.get("user_input", {}).get("custom_inputs", {})
                    selected_dimensions = prompt_data.get("selected_dimensions", {})
                    count = prompt_data.get("requirements", {}).get("count", 5)
                    target_region = custom_inputs.get("target_region", "è¶Šå—")
                    
                    # æ„å»ºç»´åº¦ä¿¡æ¯
                    dimension_summary = []
                    for dim_name, options in selected_dimensions.items():
                        if options:
                            option_names = [opt.get("name", "") for opt in options]
                            dimension_summary.append(f"{dim_name}: {', '.join(option_names)}")
                    
                    # GPT-5æ ¼å¼è¯¦ç»†åˆ›æ„ç”Ÿæˆprompt
                    gpt5_prompt = f"""è¯·ç”Ÿæˆ{count}ç»„åœ¨è¶Šå—è½åœ°çš„{user_idea}é™æ€å›¾ç‰‡å¹¿å‘Šåˆ›æ„ã€‚æ¯ç»„åˆ›æ„éƒ½éœ€è¦åŒ…å«ä¸°å¯Œè¯¦ç»†çš„å†…å®¹ï¼š

â€¢ æ ¸å¿ƒæ¦‚å¿µï¼šåˆ›æ„çš„æ ¸å¿ƒæ€æƒ³å’Œå–ç‚¹
â€¢ å»ºè®®å›¾ç‰‡çš„ç”»é¢ï¼šè¯¦ç»†çš„è§†è§‰æè¿°ï¼ŒåŒ…æ‹¬åœºæ™¯ã€äººç‰©ã€ç‰©å“ç­‰
â€¢ é•œå¤´/å…‰çº¿å¤„ç†ï¼šæ‹æ‘„è§’åº¦ã€å…‰çº¿è®¾ç½®ã€æ„å›¾ç­‰ä¸“ä¸šç»†èŠ‚
â€¢ è‰²å½©ä¸é“å…·ç­‰ç»†èŠ‚ï¼šå…·ä½“çš„è‰²å½©æ­é…ã€é“å…·é€‰æ‹©ã€æè´¨è´¨æ„Ÿç­‰
â€¢ å…³é”®æ³¨æ„äº‹é¡¹ï¼šé‡è¦æé†’ï¼ˆç»Ÿä¸€å¼ºè°ƒï¼šç”»é¢ä¸­ä¸¥ç¦å‡ºç°ä»»ä½•æ–‡å­—ã€Logoã€å­—å¹•ä¸æ ‡è¯†ï¼‰

ç”¨æˆ·æ¸¸æˆèƒŒæ™¯ï¼š{user_idea}

è¯·ç¡®ä¿æ¯ä¸ªåˆ›æ„éƒ½æœ‰ç‹¬ç‰¹æ€§å’Œä¸°å¯Œçš„ç»†èŠ‚ï¼Œè®©AIå›¾ç‰‡ç”Ÿæˆå·¥å…·èƒ½å¤Ÿå‡†ç¡®ç†è§£å¹¶ç”Ÿæˆé«˜è´¨é‡çš„å¹¿å‘Šå›¾ç‰‡ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "creatives": [
    {{
      "core_concept": "æ ¸å¿ƒæ¦‚å¿µ",
      "scene_description": "å»ºè®®å›¾ç‰‡çš„ç”»é¢æè¿°",
      "camera_lighting": "é•œå¤´/å…‰çº¿å¤„ç†",
      "color_props": "è‰²å½©ä¸é“å…·ç­‰ç»†èŠ‚",
      "key_notes": "å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆç»Ÿä¸€å¼ºè°ƒï¼šç”»é¢ä¸­ä¸¥ç¦å‡ºç°ä»»ä½•æ–‡å­—ã€Logoã€å­—å¹•ä¸æ ‡è¯†ï¼‰"
    }}
  ]
}}"""
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿprompt - ä¹Ÿéœ€è¦ä¼˜åŒ–
                    original_prompt = messages[1]['content'] if len(messages) > 1 else messages[0]['content']
                    gpt5_prompt = f"è¯·è¯¦ç»†å®Œæˆä»¥ä¸‹åˆ›æ„ç”Ÿæˆä»»åŠ¡ï¼š\n\n{original_prompt}\n\nè¯·ç¡®ä¿æ¯ä¸ªåˆ›æ„éƒ½åŒ…å«ä¸°å¯Œçš„ç»†èŠ‚å’Œç‹¬ç‰¹æ€§ï¼Œå¸®åŠ©ç”Ÿæˆé«˜è´¨é‡çš„å¹¿å‘Šå†…å®¹ã€‚"
                
                # å°è¯•GPT-5 responses.create - ä½¿ç”¨minimal reasoningç¡®ä¿å¿«é€Ÿå“åº”
                gpt5_result = self.gpt5_responses_create(
                    input_text=gpt5_prompt,
                    model=model,
                    instructions="ä½ æ˜¯ä¸“ä¸šçš„æ¸¸æˆå¹¿å‘Šå›¾ç‰‡åˆ›æ„ä¸“å®¶ã€‚è¯·ç”Ÿæˆå†…å®¹ä¸°å¯Œã€ç»†èŠ‚è¯¦å°½çš„åˆ›æ„æè¿°ï¼Œç¡®ä¿æ¯ä¸ªåˆ›æ„éƒ½æœ‰ç‹¬ç‰¹æ€§å’Œä¸“ä¸šæ€§ã€‚ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼ã€‚",
                    reasoning_effort='minimal'  # åˆ›æ„ç”Ÿæˆä½¿ç”¨minimalä»¥åŠ å¿«é€Ÿåº¦
                )
                
                if gpt5_result['success'] and gpt5_result['content'] and gpt5_result['content'].strip():
                    logger.info(f"âœ… GPT-5æ¨¡å‹æˆåŠŸ: {model}")
                    logger.info(f"ğŸ“„ GPT-5è¿”å›å†…å®¹é•¿åº¦: {len(gpt5_result['content'])}å­—ç¬¦")
                    logger.info(f"ğŸ“„ GPT-5è¿”å›å†…å®¹é¢„è§ˆ: {gpt5_result['content'][:300]}...")
                    return gpt5_result
                else:
                    logger.warning(f"âš ï¸  GPT-5æ¨¡å‹è¿”å›ç©ºå†…å®¹: {model}")
                    if gpt5_result.get('content'):
                        logger.warning(f"    å†…å®¹é•¿åº¦: {len(gpt5_result['content'])}å­—ç¬¦")
                        logger.warning(f"    å†…å®¹é¢„è§ˆ: '{gpt5_result['content'][:100]}'")
                    
            except Exception as e:
                logger.warning(f"âš ï¸  GPT-5æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            
            # GPT-5å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°GPT-4o
            fallback_model = self.MODELS[model].get('fallback', 'gpt-4o-mini')
            logger.info(f"ğŸ”„ è‡ªåŠ¨é™çº§åˆ°å¤‡ç”¨æ¨¡å‹: {fallback_model}")
        else:
            fallback_model = model
        
        # ä½¿ç”¨GPT-4oç³»åˆ—æ¨¡å‹ï¼ˆä¼ ç»Ÿchat.completionsæ ¼å¼ï¼‰
        logger.info(f"ğŸ¯ ä½¿ç”¨ä¼ ç»Ÿchat.completionsæ ¼å¼: {fallback_model}")
        
        result = self.chat_completion_with_retry(
            messages=messages,
            model=fallback_model,
            max_tokens=max_tokens,
            temperature=0.8,
            response_format={"type": "json_object"} if prompt_data else None,
            max_manual_retries=2
        )
        
        if result['success']:
            result['model_used'] = f"{fallback_model} (fallback from {model})" if model.startswith('gpt-5') else fallback_model
            logger.info(f"âœ… å¤‡ç”¨æ¨¡å‹æˆåŠŸ: {result['model_used']}")
        
        return result
    
    def analyze_content(self,
                       content: str,
                       analysis_type: str = "sentiment",
                       model: str = 'gpt-5-mini') -> Dict[str, Any]:
        """
        å†…å®¹åˆ†æåŠŸèƒ½
        
        Args:
            content: è¦åˆ†æçš„å†…å®¹
            analysis_type: åˆ†æç±»å‹ (sentiment, keywords, topics, quality)
            model: ä½¿ç”¨çš„æ¨¡å‹
        
        Returns:
            Dict: åˆ†æç»“æœ
        """
        analysis_prompts = {
            'sentiment': 'åˆ†æè¿™æ®µå†…å®¹çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰',
            'keywords': 'æå–è¿™æ®µå†…å®¹çš„å…³é”®è¯',
            'topics': 'è¯†åˆ«è¿™æ®µå†…å®¹çš„ä¸»è¦è¯é¢˜',
            'quality': 'è¯„ä¼°è¿™æ®µå†…å®¹ä½œä¸ºå¹¿å‘Šåˆ›æ„çš„è´¨é‡'
        }
        
        analysis_instruction = analysis_prompts.get(analysis_type, analysis_prompts['sentiment'])
        
        messages = [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·{analysis_instruction}ã€‚
                
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ï¼š
- analysis_type: åˆ†æç±»å‹
- result: åˆ†æç»“æœ
- confidence: ç½®ä¿¡åº¦(0-1)
- explanation: åˆ†æè¯´æ˜"""
            },
            {
                "role": "user",
                "content": f"è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n\n{content}"
            }
        ]
        
        return self.chat_completion(
            messages=messages,
            model=model,
            response_format={"type": "json_object"},
            temperature=0.1,  # è¾ƒä½æ¸©åº¦ç¡®ä¿åˆ†æå‡†ç¡®æ€§
            reasoning_effort='medium'  # åˆ†æä»»åŠ¡ä½¿ç”¨ä¸­ç­‰æ¨ç†
        )
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.MODELS.get(model_name)
    
    def list_available_models(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        return self.MODELS

# åˆ›å»ºå…¨å±€å®ä¾‹
openai_service = OpenAIService()

def translate_topics_to_chinese(topics: List[str], model: str = 'gpt-5-nano') -> Dict[str, Any]:
    """
    APIæ¥å£å‡½æ•°ï¼šç¿»è¯‘è¯é¢˜ä¸ºä¸­æ–‡
    
    Args:
        topics: è¯é¢˜åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹
    
    Returns:
        dict: ç¿»è¯‘ç»“æœ
    """
    try:
        result = openai_service.translate_to_chinese(
            texts=topics,
            model=model,
            preserve_context=True
        )
        
        return result
        
    except Exception as e:
        logger.error(f"translate_topics_to_chinese error: {e}")
        return {
            'success': False,
            'error': str(e),
            'translations': topics,  # è¿”å›åŸæ–‡
            'fallback': True
        }

def generate_creative_with_ai(prompt: str, 
                            style: str = "professional",
                            model: str = 'gpt-5-mini') -> Dict[str, Any]:
    """
    APIæ¥å£å‡½æ•°ï¼šç”Ÿæˆåˆ›æ„å†…å®¹
    
    Args:
        prompt: åˆ›æ„æç¤º
        style: é£æ ¼
        model: ä½¿ç”¨çš„æ¨¡å‹
    
    Returns:
        dict: ç”Ÿæˆç»“æœ
    """
    try:
        result = openai_service.generate_creative_content(
            prompt=prompt,
            style=style,
            model=model
        )
        
        return result
        
    except Exception as e:
        logger.error(f"generate_creative_with_ai error: {e}")
        return {
            'success': False,
            'error': str(e),
            'content': None
        }

def test_openai_service() -> Dict[str, Any]:
    """æµ‹è¯•OpenAIæœåŠ¡çŠ¶æ€"""
    try:
        if not openai_service.is_available():
            return {
                'success': False,
                'message': 'OpenAIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥API Keyé…ç½®'
            }
        
        # è¿›è¡Œç®€å•çš„æµ‹è¯•è°ƒç”¨
        test_result = openai_service.translate_to_chinese(
            texts=["Hello World"],
            model='gpt-5-nano'
        )
        
        if test_result['success']:
            return {
                'success': True,
                'message': 'OpenAIæœåŠ¡æ­£å¸¸',
                'test_translation': test_result['translations'][0]
            }
        else:
            return {
                'success': False,
                'message': f'OpenAIæœåŠ¡æµ‹è¯•å¤±è´¥: {test_result["error"]}'
            }
            
    except Exception as e:
        return {
            'success': False,
            'message': f'OpenAIæœåŠ¡æµ‹è¯•å¼‚å¸¸: {str(e)}'
        }